<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Pytorch基础</title>
    <link href="/2024/07/09/Pytorch%E5%9F%BA%E7%A1%80/"/>
    <url>/2024/07/09/Pytorch%E5%9F%BA%E7%A1%80/</url>
    
    <content type="html"><![CDATA[<p>新领域，新气象。<br>    <span id="more"></span></p><h1 id="PyTorch入门"><a href="#PyTorch入门" class="headerlink" title="PyTorch入门"></a>PyTorch入门</h1><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>在PyCharm中配置PyTorch时，需要使用本地解释器而非venv虚拟环境中的解释器，否则会出现torch.cuda.is_available()为False的情况。</p><h2 id="常用操作"><a href="#常用操作" class="headerlink" title="常用操作"></a>常用操作</h2><h3 id="torch-reshape"><a href="#torch-reshape" class="headerlink" title="torch.reshape()"></a>torch.reshape()</h3><p><code>torch.reshape(input, shape)</code>：将输入Tensor变为形状为shape的Tensor</p><h2 id="数据加载"><a href="#数据加载" class="headerlink" title="数据加载"></a>数据加载</h2><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>用于存储和管理数据的类。</p><p><code>torch.utils.data.Dataset</code>是pytorch提供的抽象类。通过继承此类，可自定义另外的Dataset类。</p><p>通过重写<code>__len()__</code>和<code>__getitem__()</code>方法，可返回数据集的大小和每个数据样本及其Label。</p><p>Dataset类中一般必定存在一个成员变量，该变量是一个列表，存放数据的文件路径。</p><p><code>__getitem__()</code>方法通常返回一个Tuple，元素0为数据内容，元素1为Label。</p><blockquote><p><code>__getitem__()</code>方法无需手动调用，在变量后加[idx]即可。类似于索引访问数组。</p><p>通常，<code>__getitem__()</code>方法根据idx从文件路径列表中取出对应的路径，进而根据路径获取数据本身。</p></blockquote><p>通过运算符<code>+</code>，可以将两个Dataset作为一个列表的两个元素，构成一个2D的更大的Dataset。</p><h3 id="Dataloader"><a href="#Dataloader" class="headerlink" title="Dataloader"></a>Dataloader</h3><p>用于从Dataset中为模型加载数据的类。</p><p><code>torch.utils.data.DataLoader</code>提供了创建DataLoader对象的方法。</p><p><code>DataLoader(dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, drop_last)</code></p><blockquote><p>dataset: 数据集实例。</p><p>batch_size：批处理大小，默认为1.</p><p>shuffle：每次训练过后是否将数据集打乱。</p><p>sampler：</p><p>batch_sampler：</p><p>num_workers：进程数。</p><p>drop_last：数据集大小&#x2F;batch_size除不尽时，余数是否丢弃。</p></blockquote><p><code>DataLoader</code>实例是一个迭代器，每个item都是一个2元素元组。</p><p>元素0是一个列表，包含了batch_size个数据。</p><p>元素1是一个列表，包含了batch_size个Label。</p><h2 id="TensorBoard"><a href="#TensorBoard" class="headerlink" title="TensorBoard"></a>TensorBoard</h2><h3 id="SummaryWriter"><a href="#SummaryWriter" class="headerlink" title="SummaryWriter"></a>SummaryWriter</h3><p><code>from torch.utils.tensorboard import SummarWriter</code></p><p><code> SummaryWriter(str)</code>实例化了一个存储在<code>str</code>文件夹中的<code> SummaryWriter</code>对象。</p><p><code> SummaryWriter</code>类用于在训练过程中向文件写入各类信息。这类信息可以被TensorBoard解析并以可视化的方式呈现。</p><h4 id="add-scalar"><a href="#add-scalar" class="headerlink" title="add_scalar()"></a>add_scalar()</h4><p><code>def add_scalar(self, tag, scalar_value, global_step=None, walltime=None)</code></p><blockquote><p><code>tag</code>是对数据的标识符。</p><p><code>scalar_value</code>是要添加的数据。</p><p><code>global_step</code>是当前训练的步数。</p></blockquote><h4 id="add-image"><a href="#add-image" class="headerlink" title="add_image()"></a>add_image()</h4><p><code>def add_image(self, tag, img_tensor, global_step=None, walltime=None, dataformats=‘CHW’)</code></p><blockquote><p><code>tag</code>是对图片数据的标识符。</p><p><code>img_tensor</code>是图片数据。可以为<code>Tensor</code>、<code>ndarray</code>或<code>string</code>。</p><p>使用<code>np.array(PIL.JpegImageFile)</code>即可将PIL图片转换为<code>ndarray</code>。</p><blockquote><p>此种方式转换的ndarray的shape为（H, W, C），即三个维度分别对应高度、宽度和通道。</p><p>因此，此时add_image的dataformats参数应当为‘HWC’。</p></blockquote><p>也可用torchvision.transforms.ToTensor()将PIL图片转化为Tensor。</p><p>也可用cv.imread将PIL图片转化为CHW的ndarray。</p></blockquote><h4 id="add-graph"><a href="#add-graph" class="headerlink" title="add_graph()"></a>add_graph()</h4><p><code>add_graph(model, input_to_model=None, verbose=False)</code></p><h2 id="Transforms"><a href="#Transforms" class="headerlink" title="Transforms"></a>Transforms</h2><p><code>from torchvision import transforms</code></p><p><code>transforms</code>用于对图片数据进行变换。该模块内置了若干工具类，</p><h3 id="Compose"><a href="#Compose" class="headerlink" title="Compose()"></a><code>Compose()</code></h3><p><code>Compose(list of transforms object)</code>用于组合多个transforms对象。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">operation = transforms.Compose([transforms.CenterCrop(<span class="hljs-number">10</span>), transforms.ToTensor()])<br><span class="hljs-comment"># 对图片进行operation操作时，先将其中心裁剪，再将其转换为Tensor对象</span><br></code></pre></td></tr></table></figure><h3 id="ToTensor"><a href="#ToTensor" class="headerlink" title="ToTensor()"></a><code>ToTensor()</code></h3><p><code>ToTensor()</code>用于将对象转换为<code>Tensor</code>对象。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用PIL打开本地图片，使用ToTensor转换为Tensor对象，再用Tensorboard写入事件文件。</span><br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms<br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br>img = Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">r&quot;data/img/1.jpg&quot;</span>)<br>totensor_operation = transforms.ToTensor()<br>img_tensor = totensor_operation(img)<br>writer = SummaryWriter(<span class="hljs-string">&quot;test&quot;</span>)<br>writer.add_image(<span class="hljs-string">&quot;测试图片&quot;</span>,img_tensor)<br>writer.close()<br></code></pre></td></tr></table></figure><h3 id="ToPILImage"><a href="#ToPILImage" class="headerlink" title="ToPILImage()"></a><code>ToPILImage()</code></h3><p>将<code>ndarray</code>或<code>Tensor</code>对象转换为PILImage。</p><h3 id="Normalize"><a href="#Normalize" class="headerlink" title="Normalize()"></a><code>Normalize()</code></h3><p>使用z-score法将Tensor对象标准化。</p><p><code>transforms.Normalize(mean=list, std=list)</code></p><blockquote><p>对于mean和std，图片有几个通道，它们就是有几个元素的list。</p><p>list各元素的值需要对数据集中的每个元素的每个通道计算得出。</p></blockquote><p>输出：(原始值-均值)&#x2F;标准差</p><p>作用：将图像的数据分布转换为标准正态分布。</p><p><img src="https://yoi-note.oss-cn-chengdu.aliyuncs.com/Image/202404271416967.png" alt="image-20240427141609922"></p><h3 id="Resize"><a href="#Resize" class="headerlink" title="Resize()"></a><code>Resize()</code></h3><p>将给定的PILImage变换至指定尺寸。</p><p><code>transforms.Resize(size)</code>实例化了一个Resize操作对象。</p><blockquote><p>当size为2元素列表时，第一个元素是高，第二个元素是宽。</p><p>当size为int时，图片更小的边将被缩放至int。</p></blockquote><p><code>transforms.Resize(img)</code>用于将已实例化的缩放操作应用于PILImage。接收的参数可为PILImage，也可为Tensor。</p><h3 id="RandomCrop"><a href="#RandomCrop" class="headerlink" title="RandomCrop()"></a><code>RandomCrop()</code></h3><p>根据给定的size随机裁剪原图片。</p><p><code>transforms.RandomCrop(size)</code>实例化了一个RandomCrop对象。</p><blockquote><p>可选参数：padding：int，2元素列表或4元素列表。分别对应：</p><ul><li>在图片四周添加int像素的间隔。</li><li>在图片左右添加[0]像素间隔，上下添加[1]像素间隔。</li><li>在图片上下左右分别添加[0]、[1]、[2]、[3]像素间隔。</li></ul><p>pad_if_needed：布尔值，当裁剪大小大于图片大小时，自动添加padding。</p><p>fill：int或3元素列表，用于填充padding的像素色。分别对应：RGB（int, int, int）和RGB（r, g, b)</p></blockquote><p><code>transforms.RandomCrop(img)</code>用于执行裁剪操作。</p><h2 id="torchvision数据集"><a href="#torchvision数据集" class="headerlink" title="torchvision数据集"></a>torchvision数据集</h2><p>以<code>CIFAR10</code>数据集为例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torchvision<br><br>train_set = torchvision.datasets.CIFAR10(root=<span class="hljs-string">&quot;dataset&quot;</span>, train=<span class="hljs-literal">True</span>, download=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># root为存放数据集的根目录;train为是否训练集;download为是否从网络下载数据集。</span><br>test_set = torchvision.datasets.CIFAR10(root=<span class="hljs-string">&quot;dataset&quot;</span>, train=<span class="hljs-literal">False</span>, download=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><blockquote><p>除上述属性外，还有：</p><p><code>transform</code>属性：可传入<code>transforms</code>函数，对数据进行预处理。</p></blockquote><h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><h2 id="nn-Module"><a href="#nn-Module" class="headerlink" title="nn.Module"></a><code>nn.Module</code></h2><p><code>nn.Module</code>是所有神经网络的基类。</p><p>自定义一个神经网络类，首先要做的便是继承<code>nn.Module</code>，随后实现<code>__init__()</code>和<code>forward()</code>函数。</p><p>实现<code>__init__()</code>函数时，首先要调用<code>super().__init__()</code>。其中classname为自定义类的类名。</p><p><code>forward()</code>函数实现了前向传播。它接收一个<code>x</code>参数输入，对<code>x</code>进行一系列处理之后返回。</p><blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyNetwork</span>(nn.Module):<br> <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>     <span class="hljs-built_in">super</span>(MyNetwork, self).__init__()<br>     <span class="hljs-comment"># 将各层操作作为变量存储</span><br>     self.layerfunction1 = nn.Conv2d(...)<br>     ...<br><br> <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>     output = self.xxxfunction(x)<br>     <span class="hljs-keyword">return</span> output<br></code></pre></td></tr></table></figure></blockquote><h2 id="nn-Sequential"><a href="#nn-Sequential" class="headerlink" title="nn.Sequential"></a><code>nn.Sequential</code></h2><p><code>Sequential</code>用于保存一系列层操作，如卷积、非线性激活、池化等。类似于Transforms的Compose。</p><p><code>model = nn.Sequential(nn.Conv2d(1,20,5),nn.ReLU(),nn.Conv2d(20,64,5),nn.ReLu)</code></p><blockquote><p>输入x可省略。</p></blockquote><h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><p>卷积（Convolution）是让两个函数经过变换得到第三个函数的过程。</p><p><img src="https://yoi-note.oss-cn-chengdu.aliyuncs.com/Image/202404292040281.png" alt="image-20240429204022080"></p><p><code>torch.nn.function</code>模块中，提供了从1D到3D数据的卷积函数：<code>conv1d()</code>、<code>conv2d</code>和<code>conv3d</code>。</p><p>以<code>conv2d()</code>为例：</p><p><code>conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -&gt; Tensor</code></p><blockquote><p><code>input</code>为输入的Tensor。在CV中，这个Tensor一般是2DTensor，是Image经过ToTensor()变换的结果。</p><blockquote><p>对于input的Tensor，其形状必须满足<code>(minibatch, in_channels, iH, iW)</code></p><p><code>minibatch</code>是<code>DataLoader</code>的<code>batch_size</code>。</p><p><code>in_channels</code>是图片的通道数。</p><p><code>iH</code>、<code>iW</code>分别是图片的高和宽。</p></blockquote><p><code>weight</code>权重，又叫卷积核。</p><blockquote><img src="https://yoi-note.oss-cn-chengdu.aliyuncs.com/Image/202404292143205.png" alt="image-20240429214304139" style="zoom:50%;" /><p>卷积核就像一个遮罩，对输入图像的局部进行处理。卷积核上元素值的不同，卷积结果就不同。元素值就是一个个权重。</p><p>权重的形状必须满足<code>(out_channels, in_channels/groups, kH, kW)</code></p><p><code>out_channels</code>是输出图片的通道数。</p></blockquote><p><code>bias</code>偏置。</p><p><code>stride</code>步进。直观来讲，步进是卷积核在输入图像上方每次移动的像素数。</p><blockquote><p>步进既可以是一个数，也可以是元组(sH, sW)。前者是横向移动的元素数，后者是纵向移动的元素数。</p></blockquote><p><code>padding</code>间隔。直观来讲，对输入图片的边缘进行填充，便于对边缘像素进行卷积处理。</p><blockquote><p>间隔既可以是一个数，也可以是元组(padH, padW)。前者是横向填充的元素数，后者是纵向填充的元素数。</p></blockquote><p><code>dilation</code></p><p><code>groups</code></p></blockquote><h2 id="结构-层"><a href="#结构-层" class="headerlink" title="结构&#x2F;层"></a>结构&#x2F;层</h2><p>可以把每个函数看作一个层。</p><h3 id="卷积层（Convolution-Layer）"><a href="#卷积层（Convolution-Layer）" class="headerlink" title="卷积层（Convolution Layer）"></a>卷积层（Convolution Layer）</h3><p><code>torch.nn</code>提供了<code>torch.nn.function</code>中各功能的简化版本。</p><p><code>Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode=‘zeros’)</code></p><blockquote><p><a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">conv_arithmetic&#x2F;README.md at master · vdumoulin&#x2F;conv_arithmetic (github.com)</a></p><p>in_channels：输入通道数</p><p>out_channels：输出通道数</p><blockquote><p>即卷积核个数。</p></blockquote><p>kernel_size：卷积核尺寸。可变量，会在训练过程中自行改变。</p><p>stride：步进数</p><p>padding：边缘补充</p><p>dilation：让卷积核的各元素之间间隔一定距离。用于空洞卷积。</p><blockquote> <img src="https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/dilation.gif" alt="img" style="zoom: 33%;" /></blockquote><p>groups：用于分组卷积</p><p>bias：偏置。</p></blockquote><h3 id="池化层（Pooling-Layer）"><a href="#池化层（Pooling-Layer）" class="headerlink" title="池化层（Pooling Layer）"></a>池化层（Pooling Layer）</h3><p>池化层将一个窗口内的所有信息浓缩为一个输出。它一般在卷积后进行，输入和输出的通道相同，不可改变。因此，输出尺寸会减小，参数减小，助于减少过拟合、提高性能。</p><blockquote><p>卷积核平移的过程，可看作滑动窗口过程。卷积核就是窗口。</p></blockquote><p>对于最大池化（Max Pooling）操作，它取一个窗口内最大的值输出，然后步进。</p><p><code>MaxPool2d(kernel_size, stride=None, padding=0*, dilation=1*, return_indices=False*, ceil_mode=False)</code></p><blockquote><p>return_indices：</p><p>ceil_mode：True时，使用ceil模式（允许出界）；否则使用floor模式（出界池化丢弃）。</p><p>stride：<strong>池化核的步长默认为核大小</strong></p></blockquote><p>平均池化（Mean Pooling）操作计算窗口内平均值输出，然后步进。</p><h3 id="填充层（Padding-Layer）"><a href="#填充层（Padding-Layer）" class="headerlink" title="填充层（Padding Layer）"></a>填充层（Padding Layer）</h3><p>类似于卷积和池化操作中的<code>Padding</code>参数。不同的是，<code>Padding</code>只能填充0，而填充层可以填充其他常数。</p><h3 id="非线性激活（Non-linear-Activations）"><a href="#非线性激活（Non-linear-Activations）" class="headerlink" title="非线性激活（Non-linear Activations）"></a>非线性激活（Non-linear Activations）</h3><h4 id="线性整流函数（ReLU）"><a href="#线性整流函数（ReLU）" class="headerlink" title="线性整流函数（ReLU）"></a>线性整流函数（ReLU）</h4><p><code>torch.nn.ReLU(x)</code>：等效于<code>max(0,x)</code>。</p><blockquote><p><code>inplace</code>参数（bool）：为True则修改传入值；否则传出新值，传入值不变。</p></blockquote><h4 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h4><p><code>torch.nn.Sigmoid(x)</code>：等效于：<code>1/(1+exp(-x))</code>。</p><h3 id="正则化层（Normalization-Layer）"><a href="#正则化层（Normalization-Layer）" class="headerlink" title="正则化层（Normalization Layer）"></a>正则化层（Normalization Layer）</h3><p>用于提高网络性能。</p><p><a href="https://blog.csdn.net/qq_41915623/article/details/125984629">pytorch中对BatchNorm2d()函数的理解-CSDN博客</a></p><p><code>torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)</code></p><blockquote><p>num_features：通道数。</p><p>eps：稳定值，避免分母为0，默认为1e-5。</p><p>momentum：将历史batch的均值与方差的影响延续到当前batch。</p><p>affine：True时，给定可以学习的系数矩阵Gamma和Beta。</p></blockquote><h3 id="线性层（Linear-Layer）"><a href="#线性层（Linear-Layer）" class="headerlink" title="线性层（Linear Layer）"></a>线性层（Linear Layer）</h3><p><code>torch.nn.Linear(in_features, out_features, bias=True)</code></p><p>对输入施加线性变换：<img src="https://yoi-note.oss-cn-chengdu.aliyuncs.com/Image/202405021536109.png" alt="image-20240502153650043" style="zoom:50%;" /></p><blockquote><p><strong>feature</strong>：特征。对事物进行分类或识别的本质就是提取特征。</p><p>在神经网络中，in_features指的是输入的数组。out_features则反之。</p><p>在Linear函数中，前两个参数分别是输入数组的元素个数和输出数组的元素个数。</p></blockquote><h3 id="丢弃层（Dropout-Layer）"><a href="#丢弃层（Dropout-Layer）" class="headerlink" title="丢弃层（Dropout Layer）"></a>丢弃层（Dropout Layer）</h3><p>防止过拟合。</p><p><code>torch.nn.Dropout2d(p=0.5, inplace=False)</code></p><p>随机将某些元素按p的概率设为0。</p><h3 id="嵌入层（Embedding-Layer）"><a href="#嵌入层（Embedding-Layer）" class="headerlink" title="嵌入层（Embedding Layer）"></a>嵌入层（Embedding Layer）</h3><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>损失函数（Loss Function）用于衡量实际输出与预期输出之间的差距，并用误差值指导模型进行进一步训练学习（即反向传播）。</p><h3 id="L1Loss"><a href="#L1Loss" class="headerlink" title="L1Loss()"></a><code>L1Loss()</code></h3><p><code>torch.nn.L1Loss(input, tartget, reduction=&#39;mean&#39;)</code></p><blockquote><p>input和target的形状为（*, *)。输出一个标量。</p><p>reduction：处理方式。‘mean’则将每个元素的MAE相加并处以元素数量。‘sum’只相加。</p></blockquote><p>比较每个元素的平均绝对误差（Mean Absolute Error, MAE）。</p><blockquote><p>output &#x3D; (|x - x’|+|y - y’| + …)&#x2F;N</p></blockquote><h3 id="SmoothL1Loss"><a href="#SmoothL1Loss" class="headerlink" title="SmoothL1Loss()"></a><code>SmoothL1Loss()</code></h3><img src="https://yoi-note.oss-cn-chengdu.aliyuncs.com/Image/202405031327320.png" alt="image-20240503132740254" style="zoom: 67%;" /><p><code>torch.nn.SmoothL1Loss(reduction=&#39;mean&#39;, beta=1.0)</code></p><p>平滑版的L1Loss。</p><p>当MAE小于1时，返回MSE的0.5倍；否则返回MAE-0.5。结合了L1和MSE的部分优点，适合多数情况。</p><h3 id="MSELoss"><a href="#MSELoss" class="headerlink" title="MSELoss()"></a><code>MSELoss()</code></h3><p><code>torch.nn.MSELoss(reduction=&#39;mean&#39;)</code></p><blockquote><p>input和target的形状为（*, *)。输出一个标量。</p></blockquote><p>比较每个元素的均方差。</p><blockquote><p>output &#x3D; (|x - x’|^2+|y - y’|^2 + …)&#x2F;N</p></blockquote><h3 id="CrossEntropyLoss"><a href="#CrossEntropyLoss" class="headerlink" title="CrossEntropyLoss()"></a><code>CrossEntropyLoss()</code></h3><p><code>torch.nn.CrossEntropyLoss(weight=None, ignore_index=-100, reduction=&#39;mean&#39;, label_smoothing=0.0)</code></p><p>比较预期输出和实际输出的交叉熵。在分类问题中常用。</p><img src="https://yoi-note.oss-cn-chengdu.aliyuncs.com/Image/202405031400362.png" alt="image-20240503140023311" style="zoom:50%;" /><blockquote><p>input的形状需要是（N, C）或(C, C)，其中N为batch_size，C为分类的类别数。</p></blockquote><h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p> 对损失函数的结果调用<code>backward()</code>子方法即可。</p><p>反向传播用于计算损失函数梯度。得到梯度以后，使用优化器对参数进行更新。</p><h2 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h2><p><a href="https://pytorch.org/docs/main/optim.html">torch.optim — PyTorch main documentation</a></p><p>以<code>torch.optim.SGD()</code>为例：</p><ul><li>实例化优化器对象：<code>optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)</code><ul><li>model.parameters()：nn.Module的parameters()函数。</li><li>lr：学习速率。一般训练前期设置为大数值，训练后期设置为小数值。</li></ul></li><li>每次取数据都要设置<code>optimizer.zero_grad()</code>。</li><li>进行反向传播后，调用<code>optimizer.step()</code></li></ul><p>对dataset的每一次遍历就是一次训练过程，称为一个epoch。</p><h2 id="现有网络"><a href="#现有网络" class="headerlink" title="现有网络"></a>现有网络</h2><p>以<code>vgg16</code>为例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">vgg16_pretrained = torchvision.models.vgg16(VGG16_Weights.IMAGENET1K_FEATURES) <span class="hljs-comment"># 下载网络并赋予权重</span><br>vgg16.add_module(<span class="hljs-string">&quot;add_linear&quot;</span>, nn.Linear(<span class="hljs-number">1000</span>, <span class="hljs-number">10</span>)) <span class="hljs-comment"># 添加自己的module</span><br>vgg16.classifier[<span class="hljs-number">7</span>] = nn.Linear(<span class="hljs-number">4096</span>, <span class="hljs-number">10</span>) <span class="hljs-comment"># 修改module</span><br></code></pre></td></tr></table></figure><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><h3 id="保存"><a href="#保存" class="headerlink" title="保存"></a>保存</h3><p>方式一：<code>torch.save(model_var_name, path)</code>：将模型结构以及参数保存为文件。</p><blockquote><p>注意path添加后缀，通常为.pth。</p></blockquote><p>方式二：<code>torch.save(model_var_name.state_dict(), path)</code>：仅保存模型参数（推荐）。</p><blockquote><p>注意path添加后缀，通常为.pkl。</p></blockquote><h3 id="读取"><a href="#读取" class="headerlink" title="读取"></a>读取</h3><p><code>torch.load(path)</code>：对应保存方式一。</p><p><code>model_var_name.load_state_dict()</code>：对应保存方式二。</p><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><h4 id="定义网络结构"><a href="#定义网络结构" class="headerlink" title="定义网络结构"></a>定义网络结构</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Network.py</span><br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyNetwork</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(MyNetwork, self).__init__()<br>        self.model_layers = nn.Sequential(<br>        nn.XXX(...)<br>        nn.XXX(...)<br>            ...<br>        )<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>):<br>        output = self.model_layers(<span class="hljs-built_in">input</span>)<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">input</span><br>    <br>    <br><span class="hljs-keyword">if</span>(__name__ == <span class="hljs-string">&quot;__main__&quot;</span>):<br>    mynetwork = MyNetwork()<br>    <span class="hljs-built_in">input</span> = xxx<br>    output = mynetwork(<span class="hljs-built_in">input</span>)<br>    <span class="hljs-built_in">print</span>(output.shape)<br></code></pre></td></tr></table></figure><h4 id="获取、读取数据集-训练"><a href="#获取、读取数据集-训练" class="headerlink" title="获取、读取数据集 | 训练"></a>获取、读取数据集 | 训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets, transforms<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn, optim<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> Network <span class="hljs-keyword">import</span> *<br><br><span class="hljs-comment"># 获取数据集并读取</span><br>train_dataset = datasets.CIFAR10(root=<span class="hljs-string">&quot;dataset_root&quot;</span>, train=<span class="hljs-literal">True</span>, transform=transforms.ToTensor(), download=<span class="hljs-literal">True</span>)<br>loader = DataLoader(train_dataset, shuffle=<span class="hljs-literal">True</span>, batch_size = <span class="hljs-number">64</span>, drop_last=<span class="hljs-literal">False</span>)<br>test_dataset = datasets.CIFAR10(root=<span class="hljs-string">&quot;dataset_root&quot;</span>, download=<span class="hljs-literal">True</span>, train=<span class="hljs-literal">False</span>, transform=transforms.ToTensor())<br>test_dataset_len = <span class="hljs-built_in">len</span>(test_dataset)<br>test_loader = DataLoader(test_dataset, shuffle=<span class="hljs-literal">True</span>, drop_last=<span class="hljs-literal">False</span>)<br>total_test_loss = <span class="hljs-number">0</span><br><span class="hljs-comment"># 实例化网络</span><br>network = MyNetwork()<br>loss = nn.CrossEntropyLoss()<br>optimizer = optim.SGD(network.parameters(), lr=<span class="hljs-number">0.01</span>)<br><span class="hljs-comment"># 定义训练参数</span><br>total_train_step = <span class="hljs-number">0</span><br>epoch = <span class="hljs-number">20</span><br>total_test_step = <span class="hljs-number">0</span><br><span class="hljs-comment"># 开始训练</span><br>mynetwork.train()<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epoch):<br>    running_loss = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> imgs, targets <span class="hljs-keyword">in</span> loader:<br>    optimizer.zero_grad()<br>    output = network(imgs)<br>    loss_val = loss(output, targets) <br>    loss_val.backward()<br>    optimizer.step()<br>     total_train_step += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">if</span>(total_train_step % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;当前训练次数：&#123;&#125; | 总训练次数：&#123;&#125; | 本次训练损失值：&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(i, total_train_step, loss_val))<br></code></pre></td></tr></table></figure><h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 接上文</span><br>    mynetwork.<span class="hljs-built_in">eval</span>()<br>    correct_num = <span class="hljs-number">0</span><br><span class="hljs-keyword">with</span> torch.nograd():<br>   <span class="hljs-keyword">for</span> imgs, targets <span class="hljs-keyword">in</span> test_loader:<br>        output = mynetwork(imgs)<br>           correct_num += (targets == output.argmax(<span class="hljs-number">1</span>)).<span class="hljs-built_in">sum</span>()<br>            correct_num = correct_num.item()<br>        loss_val = loss(output, targets)<br>        total_test_loss += loss_val<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;整体损失率：&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(total_test_loss))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;整体正确率：&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">float</span>(correct_num/test_dataset_len)))<br>    torch.save(mynetwork, <span class="hljs-string">&quot;network&#123;&#125;.pth&quot;</span>.<span class="hljs-built_in">format</span>(i))<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>人工智能</tag>
      
      <tag>Python</tag>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数字化皮影戏交互系统开发日志——沙盘模块①</title>
    <link href="/2024/01/26/%E6%95%B0%E5%AD%97%E5%8C%96%E7%9A%AE%E5%BD%B1%E6%88%8F%E4%BA%A4%E4%BA%92%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E6%97%A5%E5%BF%97%E2%80%94%E2%80%94%E6%B2%99%E7%9B%98%E6%A8%A1%E5%9D%97%E2%91%A0/"/>
    <url>/2024/01/26/%E6%95%B0%E5%AD%97%E5%8C%96%E7%9A%AE%E5%BD%B1%E6%88%8F%E4%BA%A4%E4%BA%92%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E6%97%A5%E5%BF%97%E2%80%94%E2%80%94%E6%B2%99%E7%9B%98%E6%A8%A1%E5%9D%97%E2%91%A0/</url>
    
    <content type="html"><![CDATA[<p>从零开始的一次尝试。<br>    <span id="more"></span></p><h2 id="初期策划"><a href="#初期策划" class="headerlink" title="初期策划"></a>初期策划</h2><p>既然名为“数字皮影戏科普交互系统”，首要需求便是在程序内复原实物皮影戏，包括视觉观感、操作方式、音效氛围、演绎内容等。</p><p>策划期间，需要着重考虑可行性。笔者作为技术人员，将技术可行性作为策划期间的首要考虑因素。</p><p><img src="https://yoi-note.oss-cn-chengdu.aliyuncs.com/Image/202401261814028.png" alt="image-20240126181415934"></p><h3 id="基本可行性"><a href="#基本可行性" class="headerlink" title="基本可行性"></a>基本可行性</h3><p>上面是一张实物皮影戏的视频截图。可以观察到，实物皮影戏在视觉上与传统2D平面游戏的区别在于：</p><ul><li>“远虚近实”：元素离幕布越远，色调越偏向黑色，并且越来越淡，同时整体越来越大。</li><li>位于幕后：所有元素在实物皮影戏中均位于幕布之后，通过投影在另一端呈现。</li></ul><p>“远虚近实”效果涉及到Sprite的两方面：大小与颜色。前者（Sprite的大小随距离变化）只需编写脚本动态修改Sprite的localScale即可。后者（同时修改材质的颜色与透明度）同样可以使用脚本实现。但为了提前学习图形学知识，笔者这里选择编写Shader。</p><p><img src="https://yoi-note.oss-cn-chengdu.aliyuncs.com/Image/202401261822658.png" alt="image-20240126182248623"></p><p>上图展示了Shader的编写思路。材质附着在影人上时，通过_WorldSpaceCameraPos可得到L2。保持L1不变（一般是实物皮影戏里的摄像机也不会移动过），就可计算得到L3。把L3作为变量加入Shader的frag函数中，即可让材质片元随L3的变化而变化。因此，技术上可行。</p><h3 id="整体框架"><a href="#整体框架" class="headerlink" title="整体框架"></a>整体框架</h3><p>最主要的视觉视效复刻是完全可行的。接下来考虑程序的整体框架。</p><p><img src="https://yoi-note.oss-cn-chengdu.aliyuncs.com/Image/202401261826536.png" alt="image-20240126182620479"></p><h4 id="沙盘模块"><a href="#沙盘模块" class="headerlink" title="沙盘模块"></a>沙盘模块</h4><p>对于影人，各个部件就是骨骼，部件之间的转轴就是关节。现实中表演者是用几根棍子拖动影人的主要关节来使其移动的，游戏里我们可以用鼠标拖动模拟棍子拖动。对于没有棍子控制的部位，一般使用重力+惯性的方式使其移动。在Unity里，这意味着它们要附着刚体与碰撞体组件。</p><p>如果先不考虑影人的自动移动与表演，单纯把影人作为一个2D布娃娃来看的话，只需要用铰链关节把各部位连接起来就可以了。技术可行。此外，2D布娃娃也可以作为系统的一部分，让用户拖着这个布娃娃在空白的场景里自由的玩耍。这就是沙盘模块。</p><h4 id="戏剧播放模块"><a href="#戏剧播放模块" class="headerlink" title="戏剧播放模块"></a>戏剧播放模块</h4><p>既然是复原实物皮影戏，那程序肯定得能播放经典的皮影戏剧。直接放视频未免显得太敷衍，我们要搭建一套完善的皮影戏剧播放模块。在实现了沙盘模块的基础上，我们有两个选择：</p><ul><li>不沿用2D布娃娃系统，改用2D骨骼动画。这种方式比较困难，因为皮影剧目往往持续十分钟以上，如果用K帧的方式做动画的话工作量太大。</li><li>沿用2D布娃娃系统，想办法借此制作动画。</li></ul><p>一番权衡利弊，还是在沙盘模式的基础上制作剧目动画比较好。而且如果给沙盘模式加入录制功能的话，玩家也可以自己录剧目自己看，也不失为一种乐趣。</p><p>就像视频是由一帧帧图片构成的一样，数字皮影剧目里，“一帧”是由场景内众多元素的位置信息构成的。只要每隔一段极短的时间，把时间信息与当前帧所有元素的位置信息写入文件，就可以完成“剧目”的记录。播放时，读取文件即可播放剧目。</p><h4 id="图鉴模块"><a href="#图鉴模块" class="headerlink" title="图鉴模块"></a>图鉴模块</h4><p>作为科普应用，我们的系统自然也是得有图鉴模块的。图鉴模块就比较简单，一个滑动窗口+若干UI元素就完事了。当然，要做的花哨的话，也可以模仿老滚5的加载界面，点一个UI单元，右边就会呈现它的模型。</p><h2 id="编码"><a href="#编码" class="headerlink" title="编码"></a>编码</h2><p>沙盘模块做完以后，做其他模块都会比较方便，所以笔者首先进行沙盘模块的编码。</p><h3 id="踩坑"><a href="#踩坑" class="headerlink" title="踩坑"></a>踩坑</h3><h4 id="拖拽与铰链关节的配合"><a href="#拖拽与铰链关节的配合" class="headerlink" title="拖拽与铰链关节的配合"></a>拖拽与铰链关节的配合</h4><figure class="highlight c#"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c#"><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">OnMouseDown</span>()</span><br>&#123;<br>    <span class="hljs-comment">//将物体坐标转换为屏幕坐标，获取Z轴长度</span><br>    _screenPoint = Camera.main.WorldToScreenPoint(gameObject.transform.position);<br>    <span class="hljs-comment">//计算物体中心点和鼠标触发点坐标得差值</span><br>    _offset = gameObject.transform.position - Camera.main.ScreenToWorldPoint(<span class="hljs-keyword">new</span> Vector3(Input.mousePosition.x, Input.mousePosition.y, _screenPoint.z));<br>&#125;<br></code></pre></td></tr></table></figure><p>OnMouseDrag()事件函数本质上是改变Transform的position，并不是对刚体产生影响，而铰链关节的相互作用是基于刚体的。因此，使用OnMouseDrag()编写的拖拽功能，在生效时，整个影人都是静止的，完全不存在惯性。</p><p>可能是笔者才疏学浅，到目前为止没听说过基于刚体的拖拽实现。因此，我们要在保留OnMouseDrag拖拽的同时，对刚体进行处理。</p><p>如果按照现实物理的思路，应当为对象手动添加“与拖拽相关联的惯性”。在OnMouseDrag()中记录鼠标移动增量，并以此作为速度。通过Update+Lerp实现对象Rigidbody.velocity不断向鼠标移动速度逼近，以此模仿惯性。然而，这种方式实现起来还是比较困难，同时也比较耗性能。</p><p>惯性在含有铰链关节的对象上最显著的表现是：对象物体向一端加速移动时，铰链连接着的另一物体会呈现“向反方向移动”的表现。那么，是不是只要在拖动父物体时，给子物体施加反方向速度，就能模拟惯性呢？</p><figure class="highlight c#"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c#"><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">OnMouseDrag</span>()</span>&#123;<br>    Vector3 _prevPosition = _curPosition;<span class="hljs-comment">//记录此前鼠标位置</span><br>    _curScreenPoint = <span class="hljs-keyword">new</span> Vector3(Input.mousePosition.x, Input.mousePosition.y, _screenPoint.z);<br>_curPosition = Camera.main.ScreenToWorldPoint(_curScreenPoint) + _offset;<span class="hljs-comment">//坐标系转换</span><br>_velocity = InertiiaPara*(_curPosition - _prevPosition);<span class="hljs-comment">//计算速度，其中InertiiaPara表示速度系数，越大，对反方向施加的速度就越大</span><br>transform.position = _curPosition;<br>&#125;<br></code></pre></td></tr></table></figure><p>效果非常好。</p><h4 id="多相机混合"><a href="#多相机混合" class="headerlink" title="多相机混合"></a>多相机混合</h4><p>第一次用著名Unity插件Top-Down Engine时，单独UI Camera渲染UI的实现方式深深震撼了Unity初学者的心。不管怎么说，这个项目笔者都要用上这种方式。</p><p>新建一个Camera，把Audio Listener去掉（<strong>场景内通常只存在一个Audio Listener，一般附着在Main Camera上</strong>），把它的投影方式设置为正交（因为UI不需要透视）。</p><p>注意，Clear Flags要设置为Depth Only。这里的Depth指的是相机的Depth，Depth越高的相机渲染次序越靠后。即便UI位于一大堆主相机看着的Opaque物体之后，只要UI Camera的Depth大于Main Camera，UI就能好好显示。网上有个设置UICamera的教材让把Clear Flag设置为Dont Clear，属实误人子弟。</p><p>Culling Mask设置为UI，这样UI Camera就不会渲染其他不小心跑进来的东西。</p><h4 id="代码控制Scale"><a href="#代码控制Scale" class="headerlink" title="代码控制Scale"></a>代码控制Scale</h4><p>对于2D Sprite，调整Scale时，变化的基点是Pivot。例如，Pivot位于中心的正方形，Scale等比例增大时，其四条边均匀远离中心。Pivot位于下边中点的正方形，Scale等比例增大时，下边的世界坐标（这么说其实不严谨，理解就行）不会发生变化。</p><p>通过改变Sprite的Pivot（在Sprite Editor中进行），可以避免例如位于屏幕边缘的Sprite调整Scale后超出屏幕范围的情况。</p>]]></content>
    
    
    
    <tags>
      
      <tag>数字皮影戏系统</tag>
      
      <tag>开发日志</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
